{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-13T09:37:49.762381Z",
     "start_time": "2024-04-13T09:37:49.757988Z"
    }
   },
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Model\n",
    "from keras.applications import EfficientNetB0\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, BatchNormalization, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support, confusion_matrix\n",
    "from keras.utils import to_categorical\n",
    "import tensorflow as tf"
   ],
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T09:37:49.796636Z",
     "start_time": "2024-04-13T09:37:49.763384Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# One Hot encode the labels\n",
    "\n",
    "data = \"../Data/data_directory.csv\"\n",
    "\n",
    "df = pd.read_csv(data)\n",
    "\n",
    "print(len(df))\n",
    "\n",
    "# Drop the rows where Grade is I\n",
    "df = df[df[\"Grade\"] != \"I\"]\n",
    "\n",
    "df[\"Grade\"] = df[\"Grade\"].map({\"A\": 0, \"B\": 1, \"C\": 2, \"D\": 3})\n",
    "\n",
    "df[\"Pixels\"] = \"\"\n",
    "\n",
    "print(df.head())"
   ],
   "id": "8f481d9e7c6e66c6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9125\n",
      "                             Image  Grade Pixels\n",
      "0       a_IMAGE_001_left_ankle.jpg      2       \n",
      "1        a_IMAGE_001_left_calf.jpg      2       \n",
      "2  a_IMAGE_001_left_high_thigh.jpg      2       \n",
      "3   a_IMAGE_001_left_low_thigh.jpg      3       \n",
      "4  a_IMAGE_001_left_metatarsal.jpg      3       \n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T09:38:06.203735Z",
     "start_time": "2024-04-13T09:37:49.796636Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Turn the images into pixel values\n",
    "\n",
    "image_folder = \"../Data/images\"\n",
    "\n",
    "train_ratio = 0.85\n",
    "val_ratio = 0.15\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    img_path = image_folder + \"/\" + row[\"Image\"]\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.resize(img, (224, 224))\n",
    "    img = img.astype(np.float32)\n",
    "    df.at[index, \"Pixels\"] = img\n",
    "df.head()"
   ],
   "id": "f290f68bfcde7b6c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                             Image  Grade  \\\n",
       "0       a_IMAGE_001_left_ankle.jpg      2   \n",
       "1        a_IMAGE_001_left_calf.jpg      2   \n",
       "2  a_IMAGE_001_left_high_thigh.jpg      2   \n",
       "3   a_IMAGE_001_left_low_thigh.jpg      3   \n",
       "4  a_IMAGE_001_left_metatarsal.jpg      3   \n",
       "\n",
       "                                              Pixels  \n",
       "0  [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...  \n",
       "1  [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...  \n",
       "2  [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...  \n",
       "3  [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...  \n",
       "4  [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>Grade</th>\n",
       "      <th>Pixels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a_IMAGE_001_left_ankle.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>[[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a_IMAGE_001_left_calf.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>[[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a_IMAGE_001_left_high_thigh.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>[[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a_IMAGE_001_left_low_thigh.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>[[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a_IMAGE_001_left_metatarsal.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>[[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T09:38:06.208912Z",
     "start_time": "2024-04-13T09:38:06.204738Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Split the data into train, validation, and test sets\n",
    "\n",
    "# Drop the Image column\n",
    "df = df.drop(columns=[\"Image\"])"
   ],
   "id": "6581222d360e3ba9",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T09:38:07.753239Z",
     "start_time": "2024-04-13T09:38:06.209915Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Split the data\n",
    "train, test = train_test_split(df, test_size=0.15, random_state=42)\n",
    "\n",
    "train, val = train_test_split(train, test_size=0.1, random_state=42)\n",
    "\n",
    "X_train = np.array(train[\"Pixels\"].tolist())\n",
    "X_val = np.array(val[\"Pixels\"].tolist())\n",
    "X_test = np.array(test[\"Pixels\"].tolist())\n",
    "y_train = to_categorical(train[\"Grade\"].tolist())\n",
    "y_val = to_categorical(val[\"Grade\"].tolist())\n",
    "y_test = to_categorical(test[\"Grade\"].tolist())"
   ],
   "id": "691533a9b8323c2",
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T13:02:59.499785Z",
     "start_time": "2024-04-13T09:38:07.753239Z"
    }
   },
   "cell_type": "code",
   "source": [
    "efficient_net_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "x = GlobalAveragePooling2D()(efficient_net_model.output)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "predictions = Dense(4, activation='softmax')(x)\n",
    "model = Model(inputs=efficient_net_model.input, outputs=predictions)\n",
    "\n",
    "for layer in efficient_net_model.layers[-70:]:\n",
    "    layer.trainable = True\n",
    "# for layer in efficient_net_model.layers:\n",
    "#     layer.trainable = False\n",
    "    \n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, verbose=0, min_lr=0.00001)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=10, verbose=1, restore_best_weights=True)\n",
    "\n",
    "batch_size = 50\n",
    "epochs = 100\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val), callbacks=[early_stopping, reduce_lr])"
   ],
   "id": "3a89d4e65887816e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001B[1m127/127\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m507s\u001B[0m 4s/step - accuracy: 0.5844 - loss: 1.2175 - val_accuracy: 0.3940 - val_loss: 1.3720 - learning_rate: 1.0000e-04\n",
      "Epoch 2/100\n",
      "\u001B[1m127/127\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m452s\u001B[0m 4s/step - accuracy: 0.8215 - loss: 0.5318 - val_accuracy: 0.6942 - val_loss: 0.8405 - learning_rate: 1.0000e-04\n",
      "Epoch 3/100\n",
      "\u001B[1m127/127\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m452s\u001B[0m 4s/step - accuracy: 0.8592 - loss: 0.3894 - val_accuracy: 0.7383 - val_loss: 0.7612 - learning_rate: 1.0000e-04\n",
      "Epoch 4/100\n",
      "\u001B[1m127/127\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m452s\u001B[0m 4s/step - accuracy: 0.8893 - loss: 0.3012 - val_accuracy: 0.7568 - val_loss: 0.7534 - learning_rate: 1.0000e-04\n",
      "Epoch 5/100\n",
      "\u001B[1m127/127\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m451s\u001B[0m 4s/step - accuracy: 0.9177 - loss: 0.2351 - val_accuracy: 0.8051 - val_loss: 0.5969 - learning_rate: 1.0000e-04\n",
      "Epoch 6/100\n",
      "\u001B[1m127/127\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m451s\u001B[0m 4s/step - accuracy: 0.9338 - loss: 0.1781 - val_accuracy: 0.7980 - val_loss: 0.6150 - learning_rate: 1.0000e-04\n",
      "Epoch 7/100\n",
      "\u001B[1m127/127\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m451s\u001B[0m 4s/step - accuracy: 0.9431 - loss: 0.1606 - val_accuracy: 0.8193 - val_loss: 0.6408 - learning_rate: 1.0000e-04\n",
      "Epoch 8/100\n",
      "\u001B[1m127/127\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m451s\u001B[0m 4s/step - accuracy: 0.9568 - loss: 0.1269 - val_accuracy: 0.8450 - val_loss: 0.5111 - learning_rate: 1.0000e-04\n",
      "Epoch 9/100\n",
      "\u001B[1m127/127\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m452s\u001B[0m 4s/step - accuracy: 0.9610 - loss: 0.1115 - val_accuracy: 0.8279 - val_loss: 0.5717 - learning_rate: 1.0000e-04\n",
      "Epoch 10/100\n",
      "\u001B[1m127/127\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m460s\u001B[0m 4s/step - accuracy: 0.9680 - loss: 0.0907 - val_accuracy: 0.8478 - val_loss: 0.6293 - learning_rate: 1.0000e-04\n",
      "Epoch 11/100\n",
      "\u001B[1m127/127\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m452s\u001B[0m 4s/step - accuracy: 0.9676 - loss: 0.0954 - val_accuracy: 0.8321 - val_loss: 0.6259 - learning_rate: 1.0000e-04\n",
      "Epoch 12/100\n",
      "\u001B[1m127/127\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m453s\u001B[0m 4s/step - accuracy: 0.9748 - loss: 0.0713 - val_accuracy: 0.8450 - val_loss: 0.5628 - learning_rate: 1.0000e-04\n",
      "Epoch 13/100\n",
      "\u001B[1m127/127\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m452s\u001B[0m 4s/step - accuracy: 0.9770 - loss: 0.0699 - val_accuracy: 0.7923 - val_loss: 0.7580 - learning_rate: 1.0000e-04\n",
      "Epoch 14/100\n",
      "\u001B[1m127/127\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m453s\u001B[0m 4s/step - accuracy: 0.9789 - loss: 0.0616 - val_accuracy: 0.8634 - val_loss: 0.5228 - learning_rate: 2.0000e-05\n",
      "Epoch 15/100\n",
      "\u001B[1m127/127\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m453s\u001B[0m 4s/step - accuracy: 0.9867 - loss: 0.0504 - val_accuracy: 0.8578 - val_loss: 0.5294 - learning_rate: 2.0000e-05\n",
      "Epoch 16/100\n",
      "\u001B[1m127/127\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m453s\u001B[0m 4s/step - accuracy: 0.9873 - loss: 0.0394 - val_accuracy: 0.8592 - val_loss: 0.5382 - learning_rate: 2.0000e-05\n",
      "Epoch 17/100\n",
      "\u001B[1m127/127\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m453s\u001B[0m 4s/step - accuracy: 0.9857 - loss: 0.0379 - val_accuracy: 0.8691 - val_loss: 0.5411 - learning_rate: 2.0000e-05\n",
      "Epoch 18/100\n",
      "\u001B[1m127/127\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m454s\u001B[0m 4s/step - accuracy: 0.9872 - loss: 0.0393 - val_accuracy: 0.8450 - val_loss: 0.6829 - learning_rate: 2.0000e-05\n",
      "Epoch 19/100\n",
      "\u001B[1m127/127\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m454s\u001B[0m 4s/step - accuracy: 0.9849 - loss: 0.0413 - val_accuracy: 0.8578 - val_loss: 0.6134 - learning_rate: 1.0000e-05\n",
      "Epoch 20/100\n",
      "\u001B[1m127/127\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m453s\u001B[0m 4s/step - accuracy: 0.9915 - loss: 0.0326 - val_accuracy: 0.8620 - val_loss: 0.5571 - learning_rate: 1.0000e-05\n",
      "Epoch 21/100\n",
      "\u001B[1m127/127\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m453s\u001B[0m 4s/step - accuracy: 0.9875 - loss: 0.0369 - val_accuracy: 0.8649 - val_loss: 0.5383 - learning_rate: 1.0000e-05\n",
      "Epoch 22/100\n",
      "\u001B[1m127/127\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m454s\u001B[0m 4s/step - accuracy: 0.9905 - loss: 0.0351 - val_accuracy: 0.8649 - val_loss: 0.5448 - learning_rate: 1.0000e-05\n",
      "Epoch 23/100\n",
      "\u001B[1m127/127\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m454s\u001B[0m 4s/step - accuracy: 0.9910 - loss: 0.0300 - val_accuracy: 0.8549 - val_loss: 0.6066 - learning_rate: 1.0000e-05\n",
      "Epoch 24/100\n",
      "\u001B[1m127/127\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m455s\u001B[0m 4s/step - accuracy: 0.9885 - loss: 0.0378 - val_accuracy: 0.8606 - val_loss: 0.6162 - learning_rate: 1.0000e-05\n",
      "Epoch 25/100\n",
      "\u001B[1m127/127\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m455s\u001B[0m 4s/step - accuracy: 0.9917 - loss: 0.0276 - val_accuracy: 0.8649 - val_loss: 0.5921 - learning_rate: 1.0000e-05\n",
      "Epoch 26/100\n",
      "\u001B[1m127/127\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m455s\u001B[0m 4s/step - accuracy: 0.9900 - loss: 0.0300 - val_accuracy: 0.8663 - val_loss: 0.6107 - learning_rate: 1.0000e-05\n",
      "Epoch 27/100\n",
      "\u001B[1m127/127\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m455s\u001B[0m 4s/step - accuracy: 0.9912 - loss: 0.0278 - val_accuracy: 0.8506 - val_loss: 0.6262 - learning_rate: 1.0000e-05\n",
      "Epoch 27: early stopping\n",
      "Restoring model weights from the end of the best epoch: 17.\n"
     ]
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T13:03:16.766971Z",
     "start_time": "2024-04-13T13:02:59.502790Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_scores = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", model_scores[0])\n",
    "print(\"Test Accuracy:\", model_scores[1])"
   ],
   "id": "6831bbe11e38a816",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 423ms/step - accuracy: 0.8658 - loss: 0.5969\n",
      "Test Loss: 0.5694004893302917\n",
      "Test Accuracy: 0.8630136847496033\n"
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T13:03:40.364122Z",
     "start_time": "2024-04-13T13:03:16.766971Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(y_pred)"
   ],
   "id": "905c76b430840ac5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m23s\u001B[0m 478ms/step\n",
      "[[9.98653769e-01 2.42832248e-04 7.47149970e-05 1.02864951e-03]\n",
      " [1.36746335e-11 9.99993801e-01 1.17835519e-07 6.02542741e-06]\n",
      " [9.99913454e-01 6.40981016e-05 4.76905052e-06 1.76857084e-05]\n",
      " ...\n",
      " [9.99973774e-01 4.04682403e-08 6.00949625e-06 2.02651754e-05]\n",
      " [4.55996778e-05 9.99913454e-01 1.18674334e-05 2.90784137e-05]\n",
      " [8.81353140e-01 1.12751245e-01 7.96195527e-04 5.09946560e-03]]\n"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T13:03:40.383087Z",
     "start_time": "2024-04-13T13:03:40.365124Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y_pred_temp = [np.argmax(y) for y in y_pred]\n",
    "\n",
    "y_test_temp = test[\"Grade\"].tolist()\n",
    "\n",
    "\n",
    "print(classification_report(y_test_temp, y_pred_temp))"
   ],
   "id": "7ec97f5f704f1994",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.92      0.92       729\n",
      "           1       0.74      0.77      0.76       261\n",
      "           2       0.85      0.76      0.81       199\n",
      "           3       0.85      0.85      0.85        52\n",
      "\n",
      "    accuracy                           0.86      1241\n",
      "   macro avg       0.84      0.83      0.83      1241\n",
      "weighted avg       0.86      0.86      0.86      1241\n",
      "\n"
     ]
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T13:03:40.390857Z",
     "start_time": "2024-04-13T13:03:40.385091Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "print(confusion_matrix(y_test_temp, y_pred_temp))"
   ],
   "id": "342705cf46dfb3dc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[673  48   8   0]\n",
      " [ 47 202  12   0]\n",
      " [ 17  22 152   8]\n",
      " [  2   0   6  44]]\n"
     ]
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T19:24:49.812137Z",
     "start_time": "2024-04-13T19:24:49.808312Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def pixelate_image(image):\n",
    "    \"\"\"\n",
    "    This function takes in an image and returns the pixelated version of the image\n",
    "    :param image: The path to the image\n",
    "    :return: An array of the pixelated image\n",
    "    \"\"\"\n",
    "\n",
    "    img = cv2.imread(image)\n",
    "    img = cv2.resize(img, (224, 224))\n",
    "    img = img.astype(np.float32)\n",
    "    return img\n"
   ],
   "id": "562870d4c240f1b9",
   "outputs": [],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T19:22:28.082886Z",
     "start_time": "2024-04-13T19:22:09.414433Z"
    }
   },
   "cell_type": "code",
   "source": "tf.saved_model.save(model, \"waveformMedModelMulti\")",
   "id": "661c066931391122",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: waveformMedModelMulti\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: waveformMedModelMulti\\assets\n"
     ]
    }
   ],
   "execution_count": 75
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T19:25:23.707221Z",
     "start_time": "2024-04-13T19:25:14.887215Z"
    }
   },
   "cell_type": "code",
   "source": [
    "image_path = \"../Data/images/a_IMAGE_001_left_ankle.jpg\"\n",
    "\n",
    "reconstructed_model_multi = tf.saved_model.load(\"waveformMedModelMulti\")\n",
    "\n",
    "infer_multi = reconstructed_model_multi.signatures[\"serving_default\"]\n",
    "\n",
    "img = pixelate_image(image_path)\n",
    "img = np.expand_dims(img, axis=0)\n",
    "\n",
    "input_data = {\"inputs\": img}\n",
    "\n",
    "prediction_multi = infer_multi(**input_data)\n",
    "    \n",
    "prediction_multi = prediction_multi[\"output_0\"]\n",
    "\n",
    "print(prediction_multi)"
   ],
   "id": "67e26651d6cc0da9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[8.0209560e-08 4.9692658e-06 9.9996543e-01 2.9455925e-05]], shape=(1, 4), dtype=float32)\n"
     ]
    }
   ],
   "execution_count": 77
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T19:30:04.387481Z",
     "start_time": "2024-04-13T19:30:04.384289Z"
    }
   },
   "cell_type": "code",
   "source": "print(np.argmax(prediction_multi))",
   "id": "aa94bd16c116cbaa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "execution_count": 78
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f040284d803540da"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
